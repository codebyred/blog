<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="High Availability in Proxmox Cluster using Ceph Storage"><link rel=stylesheet href=/blog/styles/style.css><link rel=icon href=/blog/images/favicon.png><title>Proxmox HA and Ceph Storage — Deployment & Concepts Guide</title></head><body><div class=container><nav class=navbar><div class=logo><a href=/blog>CodebyRed</a></div><div class=links><div class=toggle><svg width="20" height="20" viewBox="-.14 0 20.03 20.03" fill="#000"><g id="moon-alt" transform="translate(-2.25 -2)"><path id="secondary" fill="#ffe01b" d="M21 12A9 9 0 013.25 14.13 6.9 6.9.0 008 16 7 7 0 0011.61 3H12a9 9 0 019 9z"/></g></svg><div class=ball></div><svg width="20" height="20" viewBox="0 0 20 20" fill="#000"><g id="sun" transform="translate(-2 -2)"><circle id="secondary" fill="#ffe01b" cx="4" cy="4" r="4" transform="translate(8 8)"/><path id="primary" d="M12 3V4M5.64 5.64l.7.7M3 12H4m1.64 6.36.7-.7M12 21V20m6.36-1.64-.7-.7M21 12H20M18.36 5.64l-.7.7M12 8a4 4 0 104 4A4 4 0 0012 8z" fill="none" stroke="#ffe01b" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"/></g></svg></div><a href=https://codebyred.github.io/blog>Homepage</a>
<a href=https://codebyred.github.io/blog/tags/>Tags</a><div class=searchWrapper><div class=searchInputContainer><input type=text id=searchInput placeholder="Search by tag..."><svg class="searchIcon" width="20" height="20" viewBox="0 0 24 24" fill="none"><path d="M10.77 18.3C9.2807 18.3 7.82485 17.8584 6.58655 17.031 5.34825 16.2036 4.38311 15.0275 3.81318 13.6516c-.56993-1.3759-.71905-2.89-.4285-4.35064.29055-1.46067 1.00771-2.80239 2.0608-3.85548s2.39481-1.77025 3.85548-2.0608c1.46064-.29055 2.97474-.14143 4.35064.4285s2.552 1.53507 3.3794 2.77337C17.8584 7.82485 18.3 9.2807 18.3 10.77 18.3 11.7588 18.1052 12.738 17.7268 13.6516s-.9331 1.7437-1.6323 2.4429C15.3953 16.7937 14.5652 17.3484 13.6516 17.7268S11.7588 18.3 10.77 18.3zm0-13.55001c-1.18669.0-2.34673.3519-3.33343 1.01119C6.44988 6.42046 5.68084 7.35754 5.22672 8.45389 4.77259 9.55025 4.65377 10.7566 4.88528 11.9205S5.68824 14.1535 6.52735 14.9926C7.36647 15.8317 8.43556 16.4032 9.59945 16.6347c1.16385.2315 2.37025.1127 3.46665-.3414C14.1624 15.8391 15.0995 15.0701 15.7588 14.0834 16.4181 13.0967 16.77 11.9367 16.77 10.75 16.77 9.15869 16.1379 7.63257 15.0126 6.50735 13.8874 5.38213 12.3613 4.74999 10.77 4.74999z" fill="#cbd5e1"/><path d="M20 20.75C19.9015 20.7504 19.8038 20.7312 19.7128 20.6934 19.6218 20.6557 19.5392 20.6001 19.47 20.53L15.34 16.4C15.2075 16.2578 15.1354 16.0697 15.1388 15.8754 15.1422 15.6811 15.221 15.4958 15.3584 15.3583 15.4958 15.2209 15.6812 15.1422 15.8755 15.1388 16.0698 15.1354 16.2578 15.2075 16.4 15.34l4.13 4.13C20.6704 19.6106 20.7493 19.8012 20.7493 20 20.7493 20.1987 20.6704 20.3893 20.53 20.53 20.4608 20.6001 20.3782 20.6557 20.2872 20.6934 20.1962 20.7312 20.0985 20.7504 20 20.75z" fill="#cbd5e1"/></svg></div><div id=searchResults></div></div></div></nav><div class=single><div class=singleHead><div class=singleHeadTexts><h1 class=singleHeadTitle>Proxmox HA and Ceph Storage — Deployment & Concepts Guide</h1><p class=singleHeadDesc>High Availability in Proxmox Cluster using Ceph Storage</p><div class=singleHeadDetail><span><a href=https://codebyred.github.io/blog/authors/redoan/>Redoan</a>
</span><a href=https://codebyred.github.io/blog/tags/proxmox/ class=singleCategory>proxmox</a>
<a href=https://codebyred.github.io/blog/tags/ceph/ class=singleCategory>ceph</a></div></div></div><div class=singleBottom><div class=singleContent><h1 id=overview>Overview</h1><p>This document explains:</p><ul><li><p>How High Availability (HA) works in Proxmox VE</p></li><li><p>The storage requirements for HA</p></li><li><p>Core Ceph architecture concepts</p></li><li><p>Ceph sizing and configuration parameters</p></li><li><p>A step-by-step guide to simulating HA in a nested environment</p></li><li><p>Extending a Ceph cluster using a standalone Ubuntu storage node</p></li></ul><p>This guide is intended for lab experimentation or foundational understanding before production deployment.</p><h1 id=proxmox-high-availability>Proxmox High Availability</h1><h2 id=what-proxmox-ha-provides>What Proxmox HA Provides</h2><p>Proxmox VE’s High Availability subsystem automatically:</p><ul><li><p>Detects node or VM failures</p></li><li><p>Migrates workloads to healthy nodes</p></li><li><p>Restarts services without manual intervention</p></li></ul><p>This ensures service continuity when hardware or host failures occur.</p><p><strong>HA Components</strong></p><table><thead><tr><th>Component</th><th>Role</th></tr></thead><tbody><tr><td><strong>pve-ha-manager</strong></td><td>Global HA controller</td></tr><tr><td><strong>pve-ha-crm</strong></td><td>Cluster Resource Manager</td></tr><tr><td><strong>pve-ha-lrm</strong></td><td>Local Resource Manager per node</td></tr></tbody></table><h2 id=ha-requirements>HA Requirements</h2><p>Two fundamental requirements must be met:</p><table><thead><tr><th>Requirement</th><th>Description</th></tr></thead><tbody><tr><td><strong>Cluster Setup</strong></td><td>Minimum 3 nodes for quorum</td></tr><tr><td><strong>Shared Storage</strong></td><td>All nodes must access VM disks</td></tr><tr><td>Without shared storage, failover is impossible.</td><td></td></tr></tbody></table><h2 id=storage-options-for-ha>Storage Options for HA</h2><table><thead><tr><th>Storage Type</th><th>Shared</th><th>Notes</th></tr></thead><tbody><tr><td>local-lvm</td><td>❌</td><td>Node-local only</td></tr><tr><td>NFS / iSCSI / SMB</td><td>✅</td><td>Simple external sharing</td></tr><tr><td>Ceph (RBD/CephFS)</td><td>✅</td><td>Distributed & fault-tolerant</td></tr><tr><td>ZFS Replication</td><td>⚠️</td><td>Not real-time shared</td></tr><tr><td>Ceph is the native production-grade solution.</td><td></td><td></td></tr></tbody></table><h1 id=ceph-architecture-concepts>Ceph Architecture Concepts</h1><h2 id=file-write-flow-cephfs>File Write Flow (CephFS)</h2><p>When writing a file:</p><ol><li><p>Client contacts MDS</p></li><li><p>MDS determines namespace placement</p></li><li><p>File split into objects (default 4MB)</p></li><li><p>Objects mapped to Placement Groups</p></li><li><p>PGs mapped to OSDs</p></li><li><p>Replication performed
Example (10 MB file):</p></li></ol><pre tabindex=0><code>report.pdf.00000000
report.pdf.00000001
report.pdf.00000002
</code></pre><h2 id=object-placement>Object Placement</h2><ul><li><p>Objects hashed via CRUSH</p></li><li><p>Assigned to PGs</p></li><li><p>PGs distributed across OSDs</p></li><li><p>Replicated across multiple disks</p></li></ul><p>Example:</p><pre tabindex=0><code>PG217 → OSD1 OSD2 OSD3
PG219 → OSD2 OSD4 OSD5
</code></pre><h2 id=file-read-flow>File Read Flow</h2><ol><li><p>Client queries MDS</p></li><li><p>MDS returns object map</p></li><li><p>Client reads objects directly from OSDs in parallel
This enables horizontal scaling.</p></li></ol><h1 id=ceph-daemons>Ceph Daemons</h1><table><thead><tr><th>Daemon</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>OSD</strong></td><td>Stores data, replication</td></tr><tr><td><strong>MON</strong></td><td>Cluster state & quorum</td></tr><tr><td><strong>MGR</strong></td><td>Metrics & dashboard</td></tr><tr><td><strong>MDS</strong></td><td>CephFS metadata</td></tr><tr><td><strong>RGW</strong></td><td>S3/Swift object gateway</td></tr></tbody></table><h1 id=deployment-methods>Deployment Methods</h1><ul><li><p>cephadm (container-based)</p></li><li><p>Rook (Kubernetes)</p></li><li><p>Ansible automation</p></li><li><p>Salt integration</p></li><li><p>Manual installation</p></li></ul><p>This guide uses cephadm on Ubuntu 22.04</p><h1 id=conceptual-comparison-with-lvm>Conceptual Comparison with LVM</h1><table><thead><tr><th>LVM</th><th>Ceph</th></tr></thead><tbody><tr><td>Physical Disk</td><td>OSD</td></tr><tr><td>Volume Group</td><td>Cluster</td></tr><tr><td>Logical Volume</td><td>Pool</td></tr><tr><td>Filesystem</td><td>CephFS</td></tr><tr><td>RAID</td><td>Replication</td></tr><tr><td>Single Node</td><td>Distributed</td></tr></tbody></table><h1 id=pool-redundancy-parameters>Pool Redundancy Parameters</h1><h2 id=size-replication-factor>size (Replication Factor)</h2><p>Defines total copies stored.</p><pre tabindex=0><code>Usable Capacity = Raw Capacity / size
</code></pre><ul><li><p>size = 3 → three copies</p></li><li><p>Tolerates two disk failures</p></li><li><p>Recommended production value</p></li></ul><h2 id=min_size-write-quorum>min_size (Write Quorum)</h2><p>Minimum replicas required to accept writes.</p><ul><li><p>Prevents data corruption</p></li><li><p>Typical:</p></li></ul><pre tabindex=0><code>min_size = size − 1
</code></pre><p>Example:</p><table><thead><tr><th>size</th><th>min_size</th></tr></thead><tbody><tr><td>3</td><td>2</td></tr></tbody></table><h1 id=hardware-requirements>Hardware Requirements</h1><h2 id=mon>MON</h2><ul><li><p>CPU: 2 cores</p></li><li><p>RAM: 2–4 GB</p></li><li><p>Storage: SSD preferred</p></li><li><p>Minimum: 3 nodes</p></li></ul><h2 id=mgr>MGR</h2><ul><li><p>CPU: 2 cores</p></li><li><p>RAM: 1–2 GB</p></li><li><p>Deploy 2 for HA</p></li></ul><h2 id=osd>OSD</h2><table><thead><tr><th>Resource</th><th>Recommended</th></tr></thead><tbody><tr><td>CPU</td><td>2–4 cores</td></tr><tr><td>RAM</td><td>4–8 GB</td></tr><tr><td>Disk</td><td>1 per OSD</td></tr><tr><td>Network</td><td>10GbE</td></tr><tr><td>Guidelines:</td><td></td></tr></tbody></table><ul><li><p>No RAID</p></li><li><p>Separate WAL/DB if possible</p></li><li><p>Raw disks preferred</p></li></ul><h2 id=network-layout>Network Layout</h2><table><thead><tr><th>Network</th><th>Purpose</th></tr></thead><tbody><tr><td>Public</td><td>Client IO</td></tr><tr><td>Cluster</td><td>Replication</td></tr></tbody></table><h1 id=example-small-cluster>Example Small Cluster</h1><table><thead><tr><th>Node</th><th>Roles</th></tr></thead><tbody><tr><td>node1</td><td>MON MGR OSD</td></tr><tr><td>node2</td><td>MON MGR OSD</td></tr><tr><td>node3</td><td>MON MGR OSD</td></tr></tbody></table><p>Provides:</p><ul><li><p>3 MON</p></li><li><p>2 MGR</p></li><li><p>6 OSD</p></li><li><p>size=3 redundancy</p></li></ul><h1 id=simulating-ha-in-nested-proxmox>Simulating HA in Nested Proxmox</h1><p>Step 1 — Create 3 Proxmox VMs</p><p>Each VM:</p><ul><li><p>OS disk</p></li><li><p>Ceph disk</p></li><li><p>Extra disks for OSDs</p></li></ul><p>Step 2 — Create Cluster</p><p>GUI:</p><pre tabindex=0><code>Datacenter → Cluster → Create
</code></pre><p>Join others via:</p><pre tabindex=0><code>Join Information
</code></pre><p>CLI:</p><pre tabindex=0><code>pvecm create cluster
pvecm add &lt;ip&gt;
pvecm status
</code></pre><p>Step 3 — Deploy Ceph
Install</p><pre tabindex=0><code>Node → Ceph → Install
</code></pre><p>Create MON/MGR</p><pre tabindex=0><code>Ceph → Monitor
Ceph → Manager
</code></pre><p>Add OSD</p><pre tabindex=0><code>Ceph → OSD → Create
</code></pre><p>Create Pool</p><pre tabindex=0><code>Ceph → Pools → Create
</code></pre><p>Optional CephFS</p><pre tabindex=0><code>CephFS → Create
Datacenter → Storage → Add
</code></pre><p>Step 4 — Enable HA</p><pre tabindex=0><code>Datacenter → HA → Add Resource
</code></pre><p>VM now migrates automatically.</p><h1 id=local-storage-vs-ceph>Local Storage vs Ceph</h1><table><thead><tr><th>Feature</th><th>local-lvm</th><th>Ceph</th></tr></thead><tbody><tr><td>Live migration</td><td>❌</td><td>✅</td></tr><tr><td>HA failover</td><td>❌</td><td>✅</td></tr><tr><td>Shared access</td><td>❌</td><td>✅</td></tr></tbody></table><h1 id=adding-ubuntu-as-storage-node>Adding Ubuntu as Storage Node</h1><h2 id=concept>Concept</h2><p>Ubuntu node joins Ceph cluster only</p><p>NOT Proxmox cluster.</p><p>Acts purely as OSD host.</p><h2 id=preparation-steps>Preparation Steps</h2><p>Install packages</p><pre tabindex=0><code>apt install ceph ceph-common ceph-volume
</code></pre><h2 id=configure-networking>Configure networking</h2><p>Populate /etc/hosts</p><h2 id=open-firewall-ports>Open firewall ports</h2><pre tabindex=0><code>6789
3300
6800-7300
</code></pre><p>Copy config + keys</p><pre tabindex=0><code>ceph.conf
admin.keyring
bootstrap-osd.keyring
</code></pre><p>Place under:</p><pre tabindex=0><code>/etc/ceph
/var/lib/ceph/bootstrap-osd
</code></pre><h2 id=modify-config-paths>Modify config paths</h2><p>Comment Proxmox-specific keyring paths.</p><h2 id=create-osd>Create OSD</h2><pre tabindex=0><code>ceph-volume lvm create --data /dev/sdb
</code></pre><p>Node becomes active storage provider.</p><h1 id=version-compatibility-note>Version Compatibility Note</h1><p>Ceph version mismatches may occur when:</p><ul><li><p>Proxmox uses packaged Ceph</p></li><li><p>Ubuntu uses newer libraries</p></li></ul><p>Always align versions where possible.</p><h1 id=conclusion>Conclusion</h1><p>This guide demonstrated:</p><ul><li><p>HA fundamentals in Proxmox</p></li><li><p>Why shared storage is mandatory</p></li><li><p>Internal workings of Ceph distribution</p></li><li><p>Deployment practices</p></li><li><p>Cluster scaling using external nodes</p></li></ul><p>Together, Proxmox HA and Ceph provide:</p><ul><li><p>Automatic failover</p></li><li><p>Distributed storage</p></li><li><p>Horizontal scalability</p></li><li><p>Enterprise-grade resilience</p></li></ul></div><div class=singleRightBar></div></div></div><div class=footer><div class=footerLinks><a href=/>codebyred</a> |
<a href=/categories>categories</a> |
<a href=/sitemap.xml>sitemap</a></div><div class=social><a href=https://www.linkedin.com/in/md-nazmul-haque-redoan-b12a342a7/ aria-label=Facebook><svg width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><rect width="24" height="24" rx="3" fill="#0a66c2"/><path d="M6.94 8.5c.93.0 1.56-.64 1.56-1.44C8.48 6.24 7.87 5.62 6.96 5.62s-1.54.62-1.54 1.44C5.42 7.86 6.03 8.5 6.92 8.5H6.94zM5.7 9.75H8.18V18H5.7V9.75zm4.2.0h2.38v1.12H12.31C12.64 10.25 13.45 9.6 14.66 9.6c2.52.0 2.99 1.63 2.99 3.75V18H15.17V13.77C15.17 12.77 15.15 11.49 13.78 11.49c-1.39.0-1.6 1.08-1.6 2.21V18H9.9V9.75z" fill="#fff"/></svg>
</a><a href=https://twitter.com/lamawebdev aria-label=Twitter><svg width="24" height="24" viewBox="0 0 24 24" fill="currentcolor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 0C5.372.0.0 5.372.0 12c0 5.303 3.438 9.8 8.205 11.385C8.805 23.495 9.025 23.138 9.025 22.828 9.025 22.55 9.015 21.837 9.01 20.917c-3.338.648-4.042-1.615-4.042-1.615-.546-1.384-1.335-1.747-1.335-1.747C2.545 16.838 3.717 16.854 3.717 16.854 4.922 16.944 5.555 18.082 5.555 18.082c1.07 1.818 2.837 1.32 3.497 1.028C9.162 18.337 9.463 17.828 9.805 17.555 7.145 17.284 4.343 16.237 4.343 11.553c0-1.315.462-2.39 1.212-3.223C5.42 8.056 5.033 6.814 5.67 5.096c0 0 .995-.295 3.335 1.238C9.945 6.086 10.945 5.96 11.945 5.955c1 .00499999999999989 2 .131 2.94.379 2.34-1.533 3.335-1.238 3.335-1.238C18.857 6.814 18.47 8.056 18.335 8.33c.75.833 1.212 1.908 1.212 3.223.0 4.697-2.807 5.728-5.472 5.992C14.485 17.91 14.865 18.625 14.865 19.737 14.865 21.32 14.855 22.493 14.855 22.828 14.855 23.138 15.075 23.5 15.68 23.38 20.452 21.797 24 17.303 24 12 24 5.372 18.628.0 12 0z"/></svg></a></div></div></div><script src=/blog/js/app.js></script></body></html>