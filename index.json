[{"title":"Stop Tracking Files Folders in Git","description":"A step-by-step guide to safely remove tracked files/folders from Git and GitHub, even if your local branch is behind or diverged.","content":"Sometimes you accidentally commit and push files or folders (e.g., build/, dist/, public/) that should never be tracked. Later, you want to:\nStop tracking them Keep them locally for development/builds Sync your branch cleanly with remote Here’s a safe, step-by-step workflow.\nCheck Branch Status git status You may see:\nYour branch and \u0026#39;origin/main\u0026#39; have diverged, and have X and Y different commits each or modified files you don’t want to track.\nImportant: If your branch is behind remote, do not remove files yet — you will create conflicts otherwise.\nFetch Remote Changes git fetch origin This updates your local info about the remote branch.\nRebase or Merge Remote Changes If your branch diverged, rebase first to make history linear and avoid conflicts:\ngit pull --rebase origin main Resolve conflicts if they happen. If the files/folders you want to stop tracking are involved in modify/delete conflicts, you can skip the removal commit for now: git rebase --skip This ensures your branch fully incorporates all remote changes before removing the tracked folder.\nAdd to .gitignore echo \u0026#34;folder_or_file_to_ignore/\u0026#34; \u0026gt;\u0026gt; .gitignore Examples: public/, dist/, build/, node_modules/\nRemove From Git Tracking (Keep Locally) git rm -r --cached folder_or_file_to_ignore --cached removes files from Git index but keeps them on disk. Commit the Removal git add .gitignore git commit -m \u0026#34;Stop tracking folder_or_file_to_ignore\u0026#34; Push Changes git push origin main Files/folders are removed from the remote repo. Local copy remains. .gitignore prevents future accidental commits. Verify git ls-tree -r main --name-only | grep folder_or_file_to_ignore Should return nothing → removal is successful.\nKey Notes / Best Practices Always rebase first if your local branch is behind or diverged. Otherwise, Git may create modify/delete conflicts. Use .gitignore to prevent future tracking. For build/output folders, consider pushing them to a separate branch or deploying via CI/CD. Skipping a commit during rebase is safe if it’s the removal commit — you can reapply it after the rebase finishes. Why This Order Matters\nRebasing first ensures your branch has all remote commits. Removing tracked files before syncing can trigger conflicts if remote has modifications. Doing it in the correct order guarantees a clean, conflict-free commit history. ","permalink":"/blog/stop-tracking-files-folders-in-git/","tags":["git"]},{"title":"Vlan Configuration in Cisco Switches","description":"","content":"Overview On Cisco switches, VLAN configuration is primarily performed at Layer 2 (switching) using VLAN databases and per-port VLAN assignment. Layer 3 VLAN routing is performed using SVI (Switched Virtual Interface) on Layer 3 switches or routers.\nCisco separates VLAN configuration into two main components:\nSwitchport VLAN Configuration (Layer 2)\nSVI – Switched Virtual Interface (Layer 3)\nSwitchport VLAN (Layer 2 VLAN Switching) Description Switchport VLAN configuration defines:\nWhich VLAN a port belongs to\nWhether the port is Access (untagged) or Trunk (tagged multiple VLANs)\nPurpose Segregate broadcast domains\nControl VLAN tagging\nDefine access vs trunk ports\nExample Configuration Create VLAN\nvlan 10 name USERS Access Port (Like MikroTik Untagged Port)\ninterface GigabitEthernet0/1 switchport mode access switchport access vlan 10 Trunk Port (Like MikroTik Tagged Port)\ninterface GigabitEthernet0/24 switchport mode trunk switchport trunk allowed vlan 10,20,30 SVI (Switched Virtual Interface) — Layer 3 Description SVI is Cisco’s Layer 3 interface for a VLAN. It acts as:\nVLAN gateway\nRouting interface\nIP endpoint for management or routing\nExample Configuration interface vlan 10 ip address 192.168.10.1 255.255.255.0 no shutdown Requirements VLAN must exist\nAt least one active port in that VLAN\nSwitch must support Layer 3 routing (or use router-on-a-stick)\nCisco vs MikroTik Concept Mapping Cisco MikroTik Equivalent Access Port Bridge VLAN Untagged Trunk Port Bridge VLAN Tagged VLAN Database Bridge VLAN Table SVI (interface vlan X) Interface VLAN ip routing Router enabled routing Typical Cisco Deployment Models Layer 2 Switch + Router (Router-on-a-Stick)\nSwitch → VLANs L2 only Router → Subinterfaces for VLAN routing Example:\nRouter: interface g0/0.10 encapsulation dot1q 10 ip address 192.168.10.1 255.255.255.0 Layer 3 Switch (Most Modern Networks) Switch handles both L2 + L3 SVI does routing between VLANs ip routing interface vlan 10 ip address 192.168.10.1 255.255.255.0 Key Design Differences (Cisco vs MikroTik) Topic Cisco MikroTik VLAN Switching Switchport VLAN Bridge VLAN VLAN Routing SVI Interface VLAN Default Behavior VLAN exists even without ports VLAN needs bridge filtering config Hardware Switching Automatic Requires correct bridge setup Common Cisco Mistakes Creating SVI but forgetting to enable routing:\nip routing VLAN exists but no ports assigned → SVI stays down\nTrunk missing VLAN in allowed list\nSimple Mental Model Cisco Switchport Config → Controls VLAN traffic flow SVI → Gives VLAN an IP and routing MikroTik Bridge VLAN → Controls VLAN traffic flow Interface VLAN → Gives VLAN an IP and routing ","permalink":"/blog/vlan-configuration-in-cisco-switches/","tags":["cisco","switch","vlan"]},{"title":"Vlan Configuration in Mikrotik Interface Vlan vs Bridge Vlan","description":"","content":"Overview MikroTik supports VLAN implementation at both Layer 2 (Switching) and Layer 3 (Routing). Correctly distinguishing Interface VLAN and Bridge VLAN is essential for proper network segmentation, routing, and management.\nInterface VLAN (L3 VLAN Interface) Description An Interface VLAN is a logical Layer 3 interface created on top of a physical interface or bridge. It is used to assign IP addresses and route traffic for a specific VLAN.\nPurpose Provide IP addressing to VLANs\nAct as the default gateway for devices within a VLAN\nEnable inter-VLAN routing and firewall control\nExample Configuration /interface vlan add name=vlan10 vlan-id=10 interface=bridge /ip address add address=192.168.10.1/24 interface=vlan10 Use Cases\nInter-VLAN routing\nManagement VLAN IP assignment\nFirewall policies per VLAN\nBridge VLAN (L2 VLAN Filtering) Description A Bridge VLAN configuration is applied at Layer 2, inside a bridge interface. It defines which ports carry which VLANs, and whether traffic is tagged or untagged.\nPurpose Control VLAN propagation on bridge ports\nDefine tagged and untagged ports (trunk vs access)\nSegregate traffic at Layer 2 before routing\nExample Configuration /interface bridge add name=br1 vlan-filtering=yes /interface bridge port add bridge=br1 interface=ether1 add bridge=br1 interface=ether2 /interface bridge vlan add bridge=br1 vlan-ids=10 tagged=ether1 untagged=ether2 Use Cases Trunk ports connecting switches\nAccess ports for end devices\nHardware-accelerated switching and VLAN segmentation\nInterface VLAN vs Bridge VLAN: Summary Feature Interface VLAN Bridge VLAN Layer L3 L2 Assign IP / Gateway ✅ ❌ Route between VLANs ✅ ❌ Control tagged/untagged traffic ❌ ✅ Switch port segmentation ❌ ✅ Recommended Configuration Approach Configure Bridge VLANs to control Layer 2 traffic and define tagged/untagged ports.\nCreate Interface VLANs on the bridge for Layer 3 IP addressing and routing.\nExample Workflow:\nStep 1 → Bridge VLAN: Allow VLAN10 on ports ether1 (tagged) and ether2 (untagged) Step 2 → Interface VLAN: Assign IP 192.168.10.1/24 to VLAN10 interface for routing\nCommon Pitfalls Creating only Interface VLAN without updating the Bridge VLAN table may prevent VLAN traffic from reaching the router.\nForgetting to set vlan-filtering=yes on the bridge disables VLAN segregation.\n","permalink":"/blog/vlan-configuration-in-mikrotik-interface-vlan-vs-bridge-vlan/","tags":["general"]},{"title":"Lvm Snapshot Backup and Restore Guide","description":"","content":"Overview This document explains how to perform block-level backup and restore of an LVM logical volume using:\nLVM snapshots (point-in-time consistency)\ndd (block copy)\npigz (parallel compression)\nssh (secure transfer)\nThis method is suitable for:\nFull system backup\nBare-metal restore scenarios\nDisaster recovery preparation\nExact disk state preservation\nBackup Transfer Methods Two supported transfer methods are available:\nMethod A — SSH Streaming Pipeline Method B — SCP File Transfer Environment Example Component Value Volume Group ubuntu-vg Root Logical Volume ubuntu-lv Snapshot Name root_snap Backup Server 10.10.30.57 Backup Path /backup Compression Tool pigz Prerequisites Install Required Tools Backup Storage Server sudo apt update sudo apt install pigz -y Backup Target Server sudo apt install pigz -y PART 1 — Create LVM Snapshot Step 1 — Verify Volume Group Free Space sudo vgs Requirement\nSnapshot COW space must be available in VG free space. Typical planning:\nWorkload Type Recommended Snapshot Size Low write activity 5–10% of LV Medium write activity 15–20% High write activity 30%+ Step 2 — Create Snapshot sudo lvcreate -L 5G -s -n root_snap /dev/ubuntu-vg/ubuntu-lv Explanation\nOption Meaning -L 5G Snapshot COW storage size -s Create snapshot -n root_snap Snapshot name Step 3 — Verify Snapshot sudo lvs PART 2 — Backup Snapshot to Remote Server Step 4 Method-1: Stream Snapshot Using pigz + SSH\nThis method streams snapshot data directly to the remote server without creating a local backup file.\nAdvantages\nNo temporary disk usage on source\nFast for automation\nSingle pipeline command\nLess storage required on source server\nDisadvantages\nCannot resume if interrupted\nHarder to verify mid-transfer\nRequires stable network connection\nBackup Command\nsudo dd if=/dev/ubuntu-vg/root_snap bs=16M status=progress \\ | pigz -1 \\ | ssh root@10.10.30.57 \u0026#34;cat \u0026gt; /backup/root_snap.img.gz\u0026#34; Why This Pipeline Works Well\nComponent Purpose dd Reads raw block data pigz Parallel compression (faster than gzip) ssh Secure transport When To Use SSH Streaming\nBackup runs automatically (cron / automation)\nSource disk space is limited\nNetwork is reliable\nFastest transfer required\nMethod B — Backup Using SCP Transfer (File-Based Method)\nThis method creates a compressed backup file locally, then transfers it using SCP.\nAdvantages\nEasy to verify backup before transfer\nTransfer can be retried separately\nEasier troubleshooting\nAllows checksum validation\nSupports resume using rsync (if needed later)\nDisadvantages\nRequires local storage equal to compressed backup size\nTwo-step process (backup + transfer)\nFirst run this\nsudo dd if=/dev/ubuntu-vg/root_snap bs=16M status=progress \\ | pigz -1 \u0026gt; /tmp/root_snap.img.gz Verfiy Local Backup\nls -lh /tmp/root_snap.img.gz Transfer Using SCP\nscp /tmp/root_snap.img.gz root@10.10.30.57:/backup/ Step 5 — Verify Backup File ssh root@10.10.30.57 \u0026#34;ls -lh /backup/root_snap.img.gz\u0026#34; Step 6 — Remove Snapshot (Recommended) sudo lvremove -y /dev/ubuntu-vg/root_snap Why Remove Snapshot?\nSnapshots slow down disk performance and consume COW storage.\nPART 3 — Restore from Backup ⚠ CRITICAL WARNING\nRestore overwrites the entire logical volume. Restore overwrites the entire logical volume.\nPerform restore from:\nRescue Mode\nLive ISO\nMaintenance environment Never restore while system is running from that LV.\nStep 7 — Activate LVM (If Needed) sudo vgchange -ay Step 8 — Restore Snapshot Image ssh root@10.10.30.57 \u0026#34;cat /backup/root_snap.img.gz\u0026#34; \\ | pigz -d \\ | sudo dd of=/dev/ubuntu-vg/ubuntu-lv bs=16M conv=fsync status=progress Step 9 — Flush Disk Cache sync Step 10 — Reboot sudo reboot Storage Planning Guidelines Target Machine Requirement Notes VG free space ≥ snapshot size Required for COW Snapshot size Depends on write activity Backup Storage Server Requirement Notes Storage ≥ LV size per backup dd creates full image Large file support ext4 / XFS recommended CPU for compression pigz benefits from multi-core Network bandwidth Impacts backup duration Example Storage Scenario Component Size Root LV 15 GB Snapshot mapping 15 GB (virtual) Snapshot COW 5 GB Compressed backup ~7–8 GB 3 backup retention ~21–24 GB Technical Concepts LVM Snapshot — What It Is A snapshot is a point-in-time logical volume view.\nLive LV → continues changing Snapshot → frozen at creation time Copy-On-Write (COW) — How It Works When a block is modified:\n1. Original block copied to snapshot COW 2. New block written to main LV This preserves snapshot integrity.\nSnapshot Data Lifecycle Example Event Original LV Snapshot COW Stored Initial 1 1 Empty Write → 2 2 1 1 Write → 3 3 1 1 Only first change is stored.\nWhy Use dd Instead of rsync? Disk Structure Layers\nBoot Sector Partition Table Filesystem LVM Metadata Files rsync vs dd Comparison Feature rsync dd Level File Block Bootloader backup No Yes Disk layout No Yes Incremental backup Yes No Exact clone No Yes When to Use This Method Full system recovery Bare metal restore LVM infrastructure backup Golden image cloning When NOT to Use This Method Frequent incremental backups File-level restore needs Very large storage environments without deduplication Common Failure Scenarios Problem Cause Snapshot invalid COW space filled Slow system Snapshot active too long Corrupt restore Interrupted dd or network ","permalink":"/blog/lvm-snapshot-backup-and-restore-guide/","tags":["Linux","Backup"]},{"title":"Proxmox HA and Ceph Storage — Deployment \u0026 Concepts Guide","description":"High Availability in Proxmox Cluster using Ceph Storage","content":"Overview This document explains:\nHow High Availability (HA) works in Proxmox VE\nThe storage requirements for HA\nCore Ceph architecture concepts\nCeph sizing and configuration parameters\nA step-by-step guide to simulating HA in a nested environment\nExtending a Ceph cluster using a standalone Ubuntu storage node\nThis guide is intended for lab experimentation or foundational understanding before production deployment.\nProxmox High Availability What Proxmox HA Provides Proxmox VE’s High Availability subsystem automatically:\nDetects node or VM failures\nMigrates workloads to healthy nodes\nRestarts services without manual intervention\nThis ensures service continuity when hardware or host failures occur.\nHA Components\nComponent Role pve-ha-manager Global HA controller pve-ha-crm Cluster Resource Manager pve-ha-lrm Local Resource Manager per node HA Requirements Two fundamental requirements must be met:\nRequirement Description Cluster Setup Minimum 3 nodes for quorum Shared Storage All nodes must access VM disks Without shared storage, failover is impossible. Storage Options for HA Storage Type Shared Notes local-lvm ❌ Node-local only NFS / iSCSI / SMB ✅ Simple external sharing Ceph (RBD/CephFS) ✅ Distributed \u0026amp; fault-tolerant ZFS Replication ⚠️ Not real-time shared Ceph is the native production-grade solution. Ceph Architecture Concepts File Write Flow (CephFS) When writing a file:\nClient contacts MDS\nMDS determines namespace placement\nFile split into objects (default 4MB)\nObjects mapped to Placement Groups\nPGs mapped to OSDs\nReplication performed Example (10 MB file):\nreport.pdf.00000000 report.pdf.00000001 report.pdf.00000002 Object Placement Objects hashed via CRUSH\nAssigned to PGs\nPGs distributed across OSDs\nReplicated across multiple disks\nExample:\nPG217 → OSD1 OSD2 OSD3 PG219 → OSD2 OSD4 OSD5 File Read Flow Client queries MDS\nMDS returns object map\nClient reads objects directly from OSDs in parallel This enables horizontal scaling.\nCeph Daemons Daemon Purpose OSD Stores data, replication MON Cluster state \u0026amp; quorum MGR Metrics \u0026amp; dashboard MDS CephFS metadata RGW S3/Swift object gateway Deployment Methods cephadm (container-based)\nRook (Kubernetes)\nAnsible automation\nSalt integration\nManual installation\nThis guide uses cephadm on Ubuntu 22.04\nConceptual Comparison with LVM LVM Ceph Physical Disk OSD Volume Group Cluster Logical Volume Pool Filesystem CephFS RAID Replication Single Node Distributed Pool Redundancy Parameters size (Replication Factor) Defines total copies stored.\nUsable Capacity = Raw Capacity / size size = 3 → three copies\nTolerates two disk failures\nRecommended production value\nmin_size (Write Quorum) Minimum replicas required to accept writes.\nPrevents data corruption\nTypical:\nmin_size = size − 1 Example:\nsize min_size 3 2 Hardware Requirements MON CPU: 2 cores\nRAM: 2–4 GB\nStorage: SSD preferred\nMinimum: 3 nodes\nMGR CPU: 2 cores\nRAM: 1–2 GB\nDeploy 2 for HA\nOSD Resource Recommended CPU 2–4 cores RAM 4–8 GB Disk 1 per OSD Network 10GbE Guidelines: No RAID\nSeparate WAL/DB if possible\nRaw disks preferred\nNetwork Layout Network Purpose Public Client IO Cluster Replication Example Small Cluster Node Roles node1 MON MGR OSD node2 MON MGR OSD node3 MON MGR OSD Provides:\n3 MON\n2 MGR\n6 OSD\nsize=3 redundancy\nSimulating HA in Nested Proxmox Step 1 — Create 3 Proxmox VMs\nEach VM:\nOS disk\nCeph disk\nExtra disks for OSDs\nStep 2 — Create Cluster\nGUI:\nDatacenter → Cluster → Create Join others via:\nJoin Information CLI:\npvecm create cluster pvecm add \u0026lt;ip\u0026gt; pvecm status Step 3 — Deploy Ceph Install\nNode → Ceph → Install Create MON/MGR\nCeph → Monitor Ceph → Manager Add OSD\nCeph → OSD → Create Create Pool\nCeph → Pools → Create Optional CephFS\nCephFS → Create Datacenter → Storage → Add Step 4 — Enable HA\nDatacenter → HA → Add Resource VM now migrates automatically.\nLocal Storage vs Ceph Feature local-lvm Ceph Live migration ❌ ✅ HA failover ❌ ✅ Shared access ❌ ✅ Adding Ubuntu as Storage Node Concept Ubuntu node joins Ceph cluster only\nNOT Proxmox cluster.\nActs purely as OSD host.\nPreparation Steps Install packages\napt install ceph ceph-common ceph-volume Configure networking Populate /etc/hosts\nOpen firewall ports 6789 3300 6800-7300 Copy config + keys\nceph.conf admin.keyring bootstrap-osd.keyring Place under:\n/etc/ceph /var/lib/ceph/bootstrap-osd Modify config paths Comment Proxmox-specific keyring paths.\nCreate OSD ceph-volume lvm create --data /dev/sdb Node becomes active storage provider.\nVersion Compatibility Note Ceph version mismatches may occur when:\nProxmox uses packaged Ceph\nUbuntu uses newer libraries\nAlways align versions where possible.\nConclusion This guide demonstrated:\nHA fundamentals in Proxmox\nWhy shared storage is mandatory\nInternal workings of Ceph distribution\nDeployment practices\nCluster scaling using external nodes\nTogether, Proxmox HA and Ceph provide:\nAutomatic failover\nDistributed storage\nHorizontal scalability\nEnterprise-grade resilience\n","permalink":"/blog/proxmox-ceph/","tags":["proxmox","ceph"]},{"title":"Fixing Root SSH Login Failure on Ubuntu","description":"Enable remote ssh using root user and password","content":"Overview Attempting to log in as root over SSH may fail with errors such as:\nConnection closed by authenticating user root [preauth] pam_unix(sshd:auth): authentication failure Even after enabling:\nPermitRootLogin yes the login may still fail.\nRoot Cause On Ubuntu systems:\nThe root account is locked by default\nPassword authentication over SSH is often disabled\nExample Check\nsudo passwd -S root Example Output:\nroot L YYYY-MM-DD 0 99999 7 -1 Meaning:\nFlag Meaning L Locked account P Password set (Unlocked) If the account is locked, SSH password authentication will fail even if root login is permitted.\nResolution Step 1 — Enable Root Login and Password Authentication\nEdit SSH configuration:\nsudo nano /etc/ssh/sshd_config\nEnsure the following lines exist and are not commented:\nPermitRootLogin yes PasswordAuthentication yes UsePAM yes\nStep 1.1 — Check SSH Override Configuration\nSome systems override settings using included config files:\nsudo nano /etc/ssh/sshd_config.d/*.conf\nVerify these files do not override the above settings.\nStep 2 — Unlock Root Account and Set Password\nSet root password:\nsudo passwd root\nStep 2.1 — Verify Root Account Status sudo passwd -S root\nExpected Output:\nroot P YYYY-MM-DD 0 99999 7 -1\nP confirms the account is unlocked and password is set.\nStep 3 — Restart SSH Service sudo systemctl restart ssh\nStep 4 — Verify Effective SSH Configuration sshd -T | grep -E \u0026lsquo;permitrootlogin|passwordauthentication\u0026rsquo;\nExpected Output:\npermitrootlogin yes passwordauthentication yes\nValidation\nTest SSH login:\nssh root@server-ip\nSecurity Considerations (Important)\n⚠ Enabling root SSH login with password authentication is not recommended for:\nProduction systems\nInternet-facing servers\nRecommended Secure Configuration\nUse key-based or sudo-based access instead.\nPermitRootLogin prohibit-password PasswordAuthentication no\nRecommended workflow:\nssh user@server sudo -i\nBest Practice Recommendations\nUse SSH key authentication\nDisable password authentication when possible\nRestrict root login\nUse sudo for privilege escalation\nConsider fail2ban or similar protection\nSummary Issue\tCause\tFix Root SSH login fails\tRoot account locked\tSet root password PermitRootLogin yes not working\tPassword auth disabled or root locked\tEnable password auth + unlock root Authentication failure logs\tPAM + locked account\tUnlock root and verify SSH config\n","permalink":"/blog/fixing-root-ssh-login-failure-on-ubuntu/","tags":["general"]}]